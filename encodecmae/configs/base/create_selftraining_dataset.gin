INITIAL_CHECKPOINT='last'
DEVICE=[0]
GRAD_ACC=1
TRAIN_BATCH_SIZE=32
VAL_BATCH_SIZE=32
TRAIN_DATALOADER_NUM_WORKERS=8
VAL_DATALOADER_NUM_WORKERS=8
VAL_INTERVAL=1000
CHUNK_SIZE=4
MAX_AUDIO_DURATION=4
FILTER_AUDIO_LENGTH=10000

$keys_not_saved+=['datasets','dataloaders']

execute_pipeline:
    tasks = [@tasks.utils.set_seed,
             @tasks.load_model,
             @tasks.data.load_dataset,
             @tasks.data.get_dataloaders,
             @tasks.create_self_training_dataset]

tasks.load_model:
    model_dir = %UPSTREAM_DIR
    ckpt_dir = %UPSTREAM_CKPT_FILE

tasks.data.get_dataloaders.dataset_cls={'train': @train/tasks.data.DictDataset}
tasks.data.get_dataloaders.dataloader_cls={'train': @train/torch.utils.data.DataLoader}

train/torch.utils.data.DataLoader:
    shuffle=True
    batch_size=%TRAIN_BATCH_SIZE
    num_workers=%TRAIN_DATALOADER_NUM_WORKERS
    collate_fn=@tasks.data.dynamic_pad_batch
    
val/torch.utils.data.DataLoader:
    shuffle=False
    batch_size=%VAL_BATCH_SIZE
    num_workers=%VAL_DATALOADER_NUM_WORKERS
    collate_fn=@tasks.data.dynamic_pad_batch

tasks.data.DictDataset.index_mapper=@tasks.data.compensate_lengths
tasks.data.compensate_lengths.chunk_length=%MAX_AUDIO_DURATION #This will sample long audios multiple times during one epoch (duration//compensate_framing times)

tasks.data.load_dataset.filters=[@tasks.data.remove_long_audios]
tasks.data.remove_long_audios.limit=%FILTER_AUDIO_LENGTH